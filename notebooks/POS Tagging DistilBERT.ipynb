{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import indian\n",
    "import nltk\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, gc, os, typing\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW\n",
    "from torchtext import datasets\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import conllu\n",
    "from conllu import parse_incr\n",
    "from seqeval.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0+cu92'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--lr'], dest='lr', nargs=None, const=None, default=0.001, type=<class 'float'>, choices=None, help='Learning rate', metavar='')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train_data_path', type=str, metavar='', help='Path to training file')\n",
    "parser.add_argument('--valid_data_path', type=str, metavar='', help='Path to validation file')\n",
    "parser.add_argument('--base_model_type', type=str, metavar='', required=True, help='Type of transformer model. Currently supports BERT and DistilBERT.')\n",
    "parser.add_argument('--freeze', type=bool, metavar='', required=True, help='Freeze the base model if True, finetune if False', default=True)\n",
    "parser.add_argument('--batch_size', type=int, metavar='', required=True, default=32)\n",
    "parser.add_argument('--epochs', type=int, metavar='', required=True, help='Epochs to train', default=5)\n",
    "parser.add_argument('--lr', type=float, metavar='', required=True, help='Learning rate', default=0.001)\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ud_data(path:str)->tuple:\n",
    "    '''\n",
    "    Takes in the path of dataset, parses the CONLL-U format and returns 3 lists.\n",
    "    Returns\n",
    "    -all_words: list of lists, where each list has words from one example\n",
    "    -all_tags: list of lists, each list is the tag sequence for the example\n",
    "    -tags_list: list for creating tag2idx mapping.\n",
    "    \n",
    "    '''\n",
    "    data_file = open(path,'r',encoding='utf-8')\n",
    "    tl = []\n",
    "    for tokenlist in parse_incr(data_file):\n",
    "        tl.append(tokenlist)\n",
    "        \n",
    "    all_words = []\n",
    "    all_tags = []\n",
    "    tags_list = []\n",
    "    for tokenlist in tl:\n",
    "        words = []\n",
    "        tags = []\n",
    "        for i in range(len(tokenlist)):\n",
    "            words.append(tokenlist[i]['form'])\n",
    "            tags.append(tokenlist[i]['upos'])\n",
    "            tags_list.append(tokenlist[i]['upos'])\n",
    "\n",
    "        assert len(words) == len(tags)\n",
    "        all_words.append(words)\n",
    "        all_tags.append(tags)\n",
    "    \n",
    "    return all_words, all_tags, tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags, ud_train_tags = parse_ud_data('hi_hdtb-ud-train.conllu')\n",
    "dev_words, dev_tags, ud_dev_tags = parse_ud_data('hi_hdtb-ud-dev.conllu')\n",
    "\n",
    "ud_train_df = pd.DataFrame({'words':train_words, 'tags':train_tags}) \n",
    "ud_dev_df = pd.DataFrame({'words':dev_words, 'tags':dev_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.125751653638005, 116, 9885)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = [len(i) for i in train_words]\n",
    "np.mean(lens), np.max(lens), np.argmax(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ADJ': 16459,\n",
       "         'ADP': 59221,\n",
       "         'ADV': 2703,\n",
       "         'AUX': 20821,\n",
       "         'CCONJ': 5110,\n",
       "         'DET': 6081,\n",
       "         'INTJ': 3,\n",
       "         'NOUN': 62191,\n",
       "         'NUM': 5332,\n",
       "         'PART': 5610,\n",
       "         'PRON': 11857,\n",
       "         'PROPN': 34289,\n",
       "         'PUNCT': 18668,\n",
       "         'SCONJ': 5389,\n",
       "         'VERB': 27188,\n",
       "         'X': 135})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(ud_train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[यह, एशिया, की, सबसे, बड़ी, मस्जिदों, में, से,...</td>\n",
       "      <td>[DET, PROPN, ADP, ADV, ADJ, NOUN, ADP, ADP, NU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[इसे, नवाब, शाहजेहन, ने, बनवाया, था, ।]</td>\n",
       "      <td>[PRON, NOUN, PROPN, ADP, VERB, AUX, PUNCT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[इसका, प्रवेश, द्वार, दो, मंजिला, है, ।]</td>\n",
       "      <td>[PRON, NOUN, NOUN, NUM, ADJ, AUX, PUNCT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[जिसमें, चार, मेहराबें, हैं, और, मुख्य, प्रार्...</td>\n",
       "      <td>[PRON, NUM, NOUN, AUX, CCONJ, ADJ, NOUN, NOUN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[पूरी, इमारत, बेहद, खूबसूरत, है, ।]</td>\n",
       "      <td>[ADJ, NOUN, ADV, ADJ, AUX, PUNCT]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  \\\n",
       "0  [यह, एशिया, की, सबसे, बड़ी, मस्जिदों, में, से,...   \n",
       "1            [इसे, नवाब, शाहजेहन, ने, बनवाया, था, ।]   \n",
       "2           [इसका, प्रवेश, द्वार, दो, मंजिला, है, ।]   \n",
       "3  [जिसमें, चार, मेहराबें, हैं, और, मुख्य, प्रार्...   \n",
       "4                [पूरी, इमारत, बेहद, खूबसूरत, है, ।]   \n",
       "\n",
       "                                                tags  \n",
       "0  [DET, PROPN, ADP, ADV, ADJ, NOUN, ADP, ADP, NU...  \n",
       "1         [PRON, NOUN, PROPN, ADP, VERB, AUX, PUNCT]  \n",
       "2           [PRON, NOUN, NOUN, NUM, ADJ, AUX, PUNCT]  \n",
       "3  [PRON, NUM, NOUN, AUX, CCONJ, ADJ, NOUN, NOUN,...  \n",
       "4                  [ADJ, NOUN, ADV, ADJ, AUX, PUNCT]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ud_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_nktk_data():\n",
    "    '''\n",
    "    Parses NLTK data from the indian corpus.\n",
    "    '''\n",
    "    tagged_sents = indian.tagged_sents('hindi.pos')\n",
    "    all_sents = []\n",
    "    all_words = []\n",
    "    all_tags = []\n",
    "    tags_list = []\n",
    "    tagged_sents = list(tagged_sents)\n",
    "    del tagged_sents[66]\n",
    "    for i, tagged_sent in enumerate(tagged_sents):\n",
    "        sent = ''\n",
    "        tags = []\n",
    "        words = []\n",
    "        for pairs in tagged_sent:\n",
    "            if pairs[1] != '':\n",
    "                tags.append(pairs[1])\n",
    "                tags_list.append(pairs[1])\n",
    "                \n",
    "            if pairs[0] != '':\n",
    "                words.append(pairs[0])\n",
    "                \n",
    "            sent += pairs[0] + ' '\n",
    "            \n",
    "        all_tags.append(tags)\n",
    "        all_words.append(words)\n",
    "          \n",
    "        assert len(words) == len(tags)\n",
    "    \n",
    "        all_sents.append(sent)\n",
    "        \n",
    "    return all_sents, all_words, all_tags, tags_list\n",
    "\n",
    "    \n",
    "nltk_sents, nltk_words, nltk_tags, nltk_taglist = parse_nktk_data()\n",
    "nltk_df = pd.DataFrame({'sentence':nltk_sents, 'words':nltk_words, 'tags':nltk_tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>पूर्ण प्रतिबंध हटाओ : इराक</td>\n",
       "      <td>[पूर्ण, प्रतिबंध, हटाओ, :, इराक]</td>\n",
       "      <td>[JJ, NN, VFM, SYM, NNP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>संयुक्त राष्ट्र ।</td>\n",
       "      <td>[संयुक्त, राष्ट्र, ।]</td>\n",
       "      <td>[NNC, NN, SYM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>इराक के विदेश मंत्री ने अमरीका के उस प्रस्ताव ...</td>\n",
       "      <td>[इराक, के, विदेश, मंत्री, ने, अमरीका, के, उस, ...</td>\n",
       "      <td>[NNP, PREP, NNC, NN, PREP, NNP, PREP, PRP, NN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>विदेश मंत्री का कहना है कि चूंकि बगदाद संयुक्त...</td>\n",
       "      <td>[विदेश, मंत्री, का, कहना, है, कि, चूंकि, बगदाद...</td>\n",
       "      <td>[NNC, NN, PREP, VFM, VAUX, CC, CC, NNP, NNC, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>लिहाजा प्रतिबंधों को पूर्ण रूप से उठा दिया जान...</td>\n",
       "      <td>[लिहाजा, प्रतिबंधों, को, पूर्ण, रूप, से, उठा, ...</td>\n",
       "      <td>[CC, NN, PREP, JJ, NN, PREP, VFM, VAUX, VAUX, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  \\\n",
       "0                        पूर्ण प्रतिबंध हटाओ : इराक    \n",
       "1                                 संयुक्त राष्ट्र ।    \n",
       "2  इराक के विदेश मंत्री ने अमरीका के उस प्रस्ताव ...   \n",
       "3  विदेश मंत्री का कहना है कि चूंकि बगदाद संयुक्त...   \n",
       "4  लिहाजा प्रतिबंधों को पूर्ण रूप से उठा दिया जान...   \n",
       "\n",
       "                                               words  \\\n",
       "0                   [पूर्ण, प्रतिबंध, हटाओ, :, इराक]   \n",
       "1                              [संयुक्त, राष्ट्र, ।]   \n",
       "2  [इराक, के, विदेश, मंत्री, ने, अमरीका, के, उस, ...   \n",
       "3  [विदेश, मंत्री, का, कहना, है, कि, चूंकि, बगदाद...   \n",
       "4  [लिहाजा, प्रतिबंधों, को, पूर्ण, रूप, से, उठा, ...   \n",
       "\n",
       "                                                tags  \n",
       "0                            [JJ, NN, VFM, SYM, NNP]  \n",
       "1                                     [NNC, NN, SYM]  \n",
       "2  [NNP, PREP, NNC, NN, PREP, NNP, PREP, PRP, NN,...  \n",
       "3  [NNC, NN, PREP, VFM, VAUX, CC, CC, NNP, NNC, N...  \n",
       "4  [CC, NN, PREP, JJ, NN, PREP, VFM, VAUX, VAUX, ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag2idx(tags):\n",
    "    \n",
    "    tag_counter = Counter(tags)\n",
    "    tag_vocab = sorted(tag_counter, key=tag_counter.get, reverse=True)\n",
    "    print(f\"raw-vocab: {len(tag_vocab)}\")\n",
    "    \n",
    "    tag_vocab.insert(0, '[PAD]')\n",
    "    tag_vocab.insert(1, '[UNK]')\n",
    "    tag_vocab.append(\"[CLS]\")\n",
    "    tag_vocab.append(\"[SEP]\")\n",
    "    \n",
    "    print(f\"vocab-length: {len(tag_vocab)}\")\n",
    "    tag2idx = {tag:idx for idx, tag in enumerate(tag_vocab)}\n",
    "    print(f\"tag2idx-length: {len(tag2idx)}\")\n",
    "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
    "    \n",
    "    return tag2idx, idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-vocab: 16\n",
      "vocab-length: 20\n",
      "tag2idx-length: 20\n"
     ]
    }
   ],
   "source": [
    "ud_tag2idx, ud_idx2tag = create_tag2idx(ud_train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSDataset:\n",
    "    \n",
    "    def __init__(self, tokenizer, data, batch_size, tag2idx):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # divide the data into batches\n",
    "        # list of lists where each list contains batch_size number of examples\n",
    "        data = [data[i: i+batch_size] for i in range(0, len(data), batch_size)]\n",
    "        self.data = data\n",
    "        self.tag2idx = tag2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "   \n",
    "    def __iter__(self):\n",
    "        \n",
    "        max_seq_length = 300\n",
    "        \n",
    "        # iterate through batches within the data\n",
    "        for i, batch in enumerate(self.data):\n",
    "            \n",
    "            # holder lists for batches\n",
    "            _input_ids = []\n",
    "            _input_mask = []\n",
    "            _label_ids = []\n",
    "            _label_mask = []\n",
    "            _valid_ids = []\n",
    "            _segment_ids = []\n",
    "            texts = []\n",
    "            \n",
    "            # iterate through each example within a batch\n",
    "            for i in range(len(batch)):\n",
    "                word_list = batch.iloc[i].words\n",
    "                label_list = batch.iloc[i].tags\n",
    "                #text = batch.iloc[i].sentence\n",
    "                tokens = []\n",
    "                labels = []\n",
    "                valid_positions = []\n",
    "                label_mask = []\n",
    "                \n",
    "                # iterate through words of the example\n",
    "                for i, word in enumerate(word_list):\n",
    "                    \n",
    "                    \n",
    "                    # tokenize the word. Here its possible that the tokenization\n",
    "                    # returns multiple tokens due to sub words. For example\n",
    "                    # the word \"mister\" might get split into [\"mis\", \"##ter\"]\n",
    "                    token = self.tokenizer.tokenize(word)\n",
    "                    \n",
    "                    # add all the tokens to the token list.\n",
    "                    tokens.extend(token)\n",
    "                    \n",
    "                    # Extract the label for the token. If the token splits into\n",
    "                    # subwords, we'll only consider the hidden_states for the first token\n",
    "                    # and not the \"##ter\" subword. This is done by maintaining \n",
    "                    # a binary array/list of valid_positions.\n",
    "                    corresponding_label = label_list[i]\n",
    "                    \n",
    "                    for j in range(len(token)):\n",
    "                        if j == 0:\n",
    "                            labels.append(corresponding_label)\n",
    "                            valid_positions.append(1)\n",
    "                            label_mask.append(1)\n",
    "                        else:\n",
    "                            valid_positions.append(0)\n",
    "                            \n",
    "                # Create a fresh list of tokens that will finally form the input_ids for\n",
    "                # our model. We also need to prepend \"[CLS]\" token in the beginning \n",
    "                # and [SEP] token at the end of our sequence.\n",
    "                input_tokens = []\n",
    "                \n",
    "                # Not required for distilbert. Used for other models in sentence-pair tasks\n",
    "                # like QA etc.\n",
    "                segment_ids = []\n",
    "                \n",
    "                # Converting the labels/tags into their IDs.\n",
    "                label_ids = []\n",
    "                \n",
    "            \n",
    "                input_tokens.append(\"[CLS]\")\n",
    "                segment_ids.append(0)\n",
    "                valid_positions.insert(0,1)\n",
    "                \n",
    "                # label_mask is also a binary list that maintains 1 for labels and 0 for padding indices\n",
    "                # Not used in our case. For loss calculation we use ingore_index parameter, which \n",
    "                # works fine.\n",
    "                label_mask.insert(0,1)\n",
    "                label_ids.append(self.tag2idx[\"[CLS]\"])\n",
    "                \n",
    "                \n",
    "                # Transfer the tokens collected above into input_tokens after adding special tokens.\n",
    "                \n",
    "                # example: \n",
    "                # tokens = [\"Win\", \"##ter\", \"is\", \"com\", \"##ing\"]\n",
    "                # valid = [1,0,1,1,0]\n",
    "                # labels = [A,              , B,   C]\n",
    "                # the condition prevents the iteration going to the last token, i.e ##ing\n",
    "                # because it would be out of index as there are no labels for subword elements.\n",
    "                \n",
    "                for i, token in enumerate(tokens):\n",
    "                    input_tokens.append(token)\n",
    "                    segment_ids.append(0)\n",
    "                    \n",
    "                    if len(labels) > i:\n",
    "                        label_ids.append(self.tag2idx[labels[i]])\n",
    "                    \n",
    "                input_tokens.append(\"[SEP]\")\n",
    "                segment_ids.append(0)\n",
    "                valid_positions.append(1)\n",
    "                label_mask.append(1)\n",
    "                label_ids.append(self.tag2idx[\"[SEP]\"])\n",
    "                \n",
    "                # Convert the input_tokens into respective ids.\n",
    "                input_ids = self.tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "                input_mask = [1] * len(input_ids)\n",
    "                label_mask = [1] * len(input_ids)\n",
    "                \n",
    "                # sanity check\n",
    "                # The length of label_ids should equal the number of valid positions.\n",
    "                assert sum(valid_positions) == len(label_ids)\n",
    "                \n",
    "                \n",
    "                # padding\n",
    "                while len(input_ids) < max_seq_length:\n",
    "                    input_ids.append(0)\n",
    "                    input_mask.append(0)\n",
    "                    segment_ids.append(0)\n",
    "                    label_ids.append(0)\n",
    "                    valid_positions.append(0)\n",
    "                    label_mask.append(0)\n",
    "                while len(label_ids) < max_seq_length:\n",
    "                    label_ids.append(0)\n",
    "                \n",
    "                \n",
    "                assert len(input_ids) == max_seq_length\n",
    "                assert len(input_mask) == max_seq_length\n",
    "                assert len(segment_ids) == max_seq_length\n",
    "                assert len(label_ids) == max_seq_length\n",
    "                assert len(valid_positions) == max_seq_length\n",
    "                assert len(label_mask) == max_seq_length     \n",
    "                \n",
    "                \n",
    "                # stack the examples in the list\n",
    "                _input_ids.append(input_ids)\n",
    "                _input_mask.append(input_mask)\n",
    "                _label_ids.append(label_ids)\n",
    "                _label_mask.append(label_mask)\n",
    "                _valid_ids.append(valid_positions)\n",
    "                _segment_ids.append(segment_ids)\n",
    "                #texts.append(text)\n",
    "           \n",
    "            yield { \n",
    "                'input_ids':torch.tensor(_input_ids, dtype=torch.long),\n",
    "                'input_mask':torch.tensor(_input_mask, dtype=torch.long),\n",
    "                'label_ids':torch.tensor(_label_ids, dtype=torch.long),\n",
    "                'label_mask':torch.tensor(_label_mask, dtype=torch.long),\n",
    "                'valid_ids':torch.tensor(_valid_ids, dtype=torch.long),\n",
    "                'segment_ids':torch.tensor(_segment_ids, dtype=torch.long)\n",
    "                #'text':texts\n",
    "            }\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "base_model_name = ''\n",
    "if base_model_name == 'distilbert':\n",
    "    tokenizer = AutoTokenizer.from_pretrained('hi-lm-distilbert/')\n",
    "    base_model = AutoModel.from_pretrained('hi-lm-distilbert/').to(device)\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    base_model = AutoModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = POSDataset(tokenizer, ud_train_df, 32, ud_tag2idx)\n",
    "valid_dataset = POSDataset(tokenizer, ud_dev_df, 32, ud_tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.freeze == True:\n",
    "    print(\"Freezing the base model\")\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "else:\n",
    "    print(\"Finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_labels, base_model, base_model_type, freeze, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.freeze = freeze\n",
    "        self.device = device\n",
    "        self.num_labels = num_labels\n",
    "        self.base_model = base_model\n",
    "        self.base_model_type = base_model_type\n",
    "        self.fc1 = nn.Linear(768, 100)\n",
    "        self.fc2 = nn.Linear(100, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, input_mask, valid_ids, segment_ids):\n",
    "        \n",
    "       \n",
    "            \n",
    "        if self.freeze == True:\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                if self.base_model_type == 'bert':\n",
    "                    sequence_output, pooled_output = self.base_model(input_ids=input_ids, \n",
    "                                                                     attention_mask=input_mask, \n",
    "                                                                     token_type_ids=segment_ids)\n",
    "                \n",
    "                elif self.base_model_type == 'distilbert':\n",
    "                    sequence_output = self.base_model(input_ids=input_ids, attention_mask=input_mask)[0]\n",
    "        \n",
    "        else:\n",
    "            if self.base_model_type == 'bert':\n",
    "                sequence_output, pooled_output = self.base_model(input_ids=input_ids, \n",
    "                                                                 attention_mask=input_mask, \n",
    "                                                                 token_type_ids=segment_ids)\n",
    "            \n",
    "            elif self.base_model_type == 'distilbert':\n",
    "                    sequence_output = self.base_model(input_ids=input_ids, attention_mask=input_mask)[0]\n",
    "        \n",
    "            \n",
    "            \n",
    "        batch_size, max_len, feature_dim = sequence_output.shape\n",
    "        \n",
    "        valid_output = torch.zeros(batch_size, max_len, feature_dim, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            m = -1\n",
    "            for j in range(max_len):\n",
    "                if valid_ids[i][j].item() == 1:\n",
    "                    m += 1\n",
    "                    valid_output[i][m] = sequence_output[i][j]\n",
    "        \n",
    "        sequence_output = F.dropout(valid_output, p=0.3)\n",
    "        logits = self.fc2(self.fc1(sequence_output))\n",
    "        # [bs, seq_len, num_labels]\n",
    "        \n",
    "        logits = logits.view(-1, self.num_labels)\n",
    "        # [N, num_labels]\n",
    "        \n",
    "        return logits        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = POS(len(ud_tag2idx), base_model, 'bert', device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx=0):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    # preds = [N, num_labels]\n",
    "    # y = [N]\n",
    "    # N = bs * seq_len\n",
    "    \n",
    "    # Gets the index of maximum values across the first dimension.\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    # [N, 1]\n",
    "    \n",
    "    # Gets the index of elements from the ground-truth that are not 0. \n",
    "    # Basically index positions which are not padded.\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    # [num_nonzero, 1]\n",
    "    \n",
    "    # Use non_pad_elements to index the predictions and ground truth. tensor of bools.\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    # [num_nonzero, 1]\n",
    "    \n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqeval_metrics(preds, y, tag_pad_idx=0):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    # preds = [N, num_labels]\n",
    "    # y = [N]\n",
    "    # N = bs * seq_len\n",
    "    \n",
    "    # Gets the index of maximum values across the first dimension.\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    # [N, 1]\n",
    "    \n",
    "    # Gets the index of elements from the ground-truth that are not 0. \n",
    "    # Basically index positions which are not padded.\n",
    "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
    "    # [num_nonzero, 1]\n",
    "    \n",
    "    nonzero_preds = max_preds[non_pad_elements].squeeze(1).tolist()\n",
    "    nonzero_y = y[non_pad_elements].tolist()\n",
    "    \n",
    "    y_true = [[ud_idx2tag[l[0]]] for l in nonzero_y]\n",
    "    y_pred = [[ud_idx2tag[l[0]]] for l in nonzero_preds]\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return accuracy, f1\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_dataset):\n",
    "    \n",
    "    print(\"Starting Training\")\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    model.train()\n",
    "    \n",
    "    for bi, batch in enumerate(train_dataset):\n",
    "\n",
    "        if bi % 50 == 0:\n",
    "            print(f\"Starting batch: {bi}\")\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        input_mask = batch['input_mask'].to(device)\n",
    "        label_ids = batch['label_ids'].to(device)\n",
    "        valid_ids = batch['valid_ids'].to(device)\n",
    "        segment_ids = batch['segment_ids'].to(device)\n",
    "        \n",
    "        preds = model(input_ids, input_mask, valid_ids, segment_ids)\n",
    "        loss = F.cross_entropy(preds, label_ids.view(-1), ignore_index=0)\n",
    "        train_acc += categorical_accuracy(preds, label_ids.view(-1)).item()\n",
    "        #train_acc += accuracy_score(label_ids.view(-1).tolist(), preds.tolist())\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    return train_loss/len(train_dataset), train_acc/len(train_dataset)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, valid_dataset):\n",
    "    \n",
    "    print(\"Starting validation\")\n",
    "    valid_loss = 0.\n",
    "    valid_acc = 0.\n",
    "    model.eval()\n",
    "    valid_f1 = 0.\n",
    "    \n",
    "    for bi, batch in enumerate(valid_dataset):\n",
    "\n",
    "        if bi % 50 == 0:\n",
    "            print(f\"Starting batch: {bi}\")\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        input_mask = batch['input_mask'].to(device)\n",
    "        label_ids = batch['label_ids'].to(device)\n",
    "        valid_ids = batch['valid_ids'].to(device)\n",
    "        segment_ids = batch['segment_ids'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            preds = model(input_ids, input_mask, valid_ids, segment_ids)\n",
    "            loss = F.cross_entropy(preds, label_ids.view(-1), ignore_index=0)\n",
    "        \n",
    "            valid_loss += loss.item()\n",
    "            #valid_acc += categorical_accuracy(preds, label_ids.view(-1)).item()\n",
    "            acc, f1  = seqeval_metrics(preds, label_ids.view(-1))\n",
    "            valid_acc += acc\n",
    "            valid_f1 += f1\n",
    "            \n",
    "    valid_loss = valid_loss / len(valid_dataset)\n",
    "    valid_acc = valid_acc/len(valid_dataset)\n",
    "    valid_f1 = valid_f1/len(valid_dataset)\n",
    "    \n",
    "    return valid_loss, valid_acc, valid_f1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    '''\n",
    "    Helper function to record epoch time.\n",
    "    '''\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Starting Training\n",
      "Starting batch: 0\n",
      "Starting batch: 50\n",
      "Starting batch: 100\n",
      "Starting batch: 150\n",
      "Starting batch: 200\n",
      "Starting batch: 250\n",
      "Starting batch: 300\n",
      "Starting batch: 350\n",
      "Starting batch: 400\n",
      "Starting validation\n",
      "Starting batch: 0\n",
      "Starting batch: 50\n",
      "Epoch train loss : 0.5372212441064991| Time: 31m 4s\n",
      "Epoch valid loss: 0.3009880973169437\n",
      "Epoch train accuracy: 0.8339941346695503\n",
      "Epoch valid accuracy: 0.8967676059557841\n",
      "====================================================================================\n",
      "Epoch 2\n",
      "Starting Training\n",
      "Starting batch: 0\n",
      "Starting batch: 50\n",
      "Starting batch: 100\n",
      "Starting batch: 150\n",
      "Starting batch: 200\n",
      "Starting batch: 250\n",
      "Starting batch: 300\n",
      "Starting batch: 350\n",
      "Starting batch: 400\n",
      "Starting validation\n",
      "Starting batch: 0\n",
      "Starting batch: 50\n",
      "Epoch train loss : 0.3568698224038459| Time: 31m 53s\n",
      "Epoch valid loss: 0.2985829169360491\n",
      "Epoch train accuracy: 0.8791164036553639\n",
      "Epoch valid accuracy: 0.8972801623436121\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "train_accs = []\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, optimizer, train_dataset)\n",
    "    valid_loss, valid_acc, valid_f1 = validate(model, valid_dataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    #valid_losses.append(valid_loss)\n",
    "    #valid_accs.append(valid_acc)\n",
    "    #train_accs.append(train_acc)\n",
    "    \n",
    "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Epoch valid loss: {valid_loss}\")\n",
    "    print(f\"Epoch train accuracy: {train_acc}\")\n",
    "    print(f\"Epoch valid accuracy: {valid_acc}\")\n",
    "    print(f\"Epoch F1 score: {valid_f1}\")\n",
    "    print(\"====================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
